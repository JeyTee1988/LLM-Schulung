{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JeyTee1988/LLM-Schulung/blob/main/llm_schulung.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Wichtig**: Wenn Du dieses Notebook in Google Colab ausf√ºhrst, w√§hle zuerst unter Laufzeit > Laufzeittyp √§ndern eine GPU aus"
      ],
      "metadata": {
        "id": "0B_oPwRRt5g-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Zun√§chst installieren wir die ben√∂tigte Library Huggingface Transformer"
      ],
      "metadata": {
        "id": "YKQgG_RAdeVW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i2NQGZs0YPDT",
        "outputId": "8de5df42-7d33-47db-8b73-9e19fada0b26"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.31.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.12.2)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.14.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.16.4)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.22.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2022.10.31)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.27.1)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.13.3)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.3.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.65.0)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (4.7.1)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (1.26.16)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2023.7.22)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Klassifizierung von Text"
      ],
      "metadata": {
        "id": "F9h7zRqixgrZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Wir laden eine Pipeline mit einem vortrainierten Sentiment-Modell. Das Modell kann explizit angegeben werden, ansonsten wird ein Standard-Modell genommen."
      ],
      "metadata": {
        "id": "l4NGjJ5h2zj5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "classifier = pipeline(\"sentiment-analysis\")"
      ],
      "metadata": {
        "id": "hLnzl9v7xl2A",
        "outputId": "3abd566c-b975-45a5-99b3-80f9ecc1aa97",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "No model was supplied, defaulted to distilbert-base-uncased-finetuned-sst-2-english and revision af0f99b (https://huggingface.co/distilbert-base-uncased-finetuned-sst-2-english).\n",
            "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Wir k√∂nnen das Modell nutzen, um die Stimmung eines Textes zu bestimmen. Hierbei wird angegeben, ob der Text positiv oder negativ ist und wie sicher das Modell ist bei der Bewertung."
      ],
      "metadata": {
        "id": "dj4v6DtZ3LfC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "classifier([\"We are happy to have you participate in our modest LLM workshop!\", \"Sorry that we had to cap the participant limit to 20\"])"
      ],
      "metadata": {
        "id": "b98V2LgUx5Vx",
        "outputId": "0e19b466-60ad-4513-b06e-d544ca45f49d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'label': 'POSITIVE', 'score': 0.9998558759689331},\n",
              " {'label': 'NEGATIVE', 'score': 0.9991243481636047}]"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Dieses Modell wurde nur mit englischen Texten und scheinbar nicht mit Emoji trainiert:"
      ],
      "metadata": {
        "id": "qee7vBWNzTJ6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "classifier([\"Was ein sch√∂ner Tag f√ºr eine Schulung\", \"ü§ó\", \"üò†\"])"
      ],
      "metadata": {
        "id": "m5KZgde0zYJp",
        "outputId": "d49a83fd-ce50-4a9b-d2c5-ed5a8c5ffa8a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'label': 'NEGATIVE', 'score': 0.9137821793556213},\n",
              " {'label': 'NEGATIVE', 'score': 0.6970565319061279},\n",
              " {'label': 'NEGATIVE', 'score': 0.6970565319061279}]"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Textgenerierung"
      ],
      "metadata": {
        "id": "jQCRenrNxWmr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Wir laden das vortrainierte distilgpt2 Modell und nutzen es f√ºr Textgenerierung"
      ],
      "metadata": {
        "id": "LiMZwctsdoXb"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "RdHcyatVTaqq"
      },
      "outputs": [],
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "pipe = pipeline(\"text-generation\", model=\"distilgpt2\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Wir k√∂nnen das Modell nutzen, um Texte zu vervollst√§ndigen."
      ],
      "metadata": {
        "id": "ietmuiGteF4G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import set_seed\n",
        "set_seed(42)\n",
        "pipe(\"Hello, I‚Äôm a language model\", max_length=20, num_return_sequences=5)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vooe3wYOYiHM",
        "outputId": "f9666043-389b-4e77-88ab-2b1dc428ae56"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'generated_text': 'Hello, I‚Äôm a language model but what I do in that role is to make everything'},\n",
              " {'generated_text': 'Hello, I‚Äôm a language model.‚Äù You‚Äôll know the real name'},\n",
              " {'generated_text': 'Hello, I‚Äôm a language model‚Äôs\\xadself, and I‚Äôm'},\n",
              " {'generated_text': 'Hello, I‚Äôm a language model, and I really wanted to make a nice API,'},\n",
              " {'generated_text': 'Hello, I‚Äôm a language model. I‚Äôthink in any language. IÔøΩ'}]"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ein Tokenizer in einem Language Learning Model (LLM) dient dazu, Text in kleinere Einheiten, sogenannte Tokens, zu zerlegen. Diese Tokens k√∂nnen einzelne W√∂rter, Phrasen oder sogar einzelne Buchstaben sein.\n",
        "\n",
        "Der Tokenizer ist wichtig, weil ein LLM wie ein neuronales Netzwerk arbeitet, das auf Zahlen, nicht auf W√∂rtern, basiert. Es kann nicht direkt Text verstehen oder interpretieren. Stattdessen muss es den Text in eine numerische Form umwandeln, die es verarbeiten kann.\n",
        "\n",
        "Dieser Prozess des Zerlegens und Umwandeln wird durch den Tokenizer durchgef√ºhrt. Er nimmt den urspr√ºnglichen Text, teilt ihn in Tokens und wandelt diese dann in Zahlen um, die das Modell verarbeiten kann.\n",
        "\n",
        "https://platform.openai.com/tokenizer\n"
      ],
      "metadata": {
        "id": "rIcp-u4CdZm9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = pipe.tokenizer\n",
        "model = pipe.model\n",
        "\n",
        "text = \"Hello, I‚Äôm a language model\"\n",
        "tokenizer.tokenize(text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i9eFFJlca55Q",
        "outputId": "3b411b44-e82e-4a6c-a91c-324a24f1be5f"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Hello', ',', 'ƒ†I', '√¢ƒ¢', 'ƒª', 'm', 'ƒ†a', 'ƒ†language', 'ƒ†model']"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "encoded_input = tokenizer(text, return_tensors='pt')\n",
        "encoded_input.input_ids"
      ],
      "metadata": {
        "id": "ciBX-ajY7QKN",
        "outputId": "ab1027c0-3582-419a-9733-a88e93c9df9a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[15496,    11,   314,   447,   247,    76,   257,  3303,  2746]])"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**ToDo: Verstehen**"
      ],
      "metadata": {
        "id": "4Q5EaoFX4TQp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "output = model(**encoded_input)\n",
        "output.logits.size()\n",
        "output"
      ],
      "metadata": {
        "id": "C_Qw-qGqu5ac"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Weitere Arten von Tasks\n",
        "Auf folgender Seite findet ihr weitere m√∂gliche Anwendungsf√§lle\n",
        "https://huggingface.co/docs/transformers/main/en/quicktour#pipeline\n",
        "\n",
        "Probiert gerne welche aus ü§ó"
      ],
      "metadata": {
        "id": "WzoLxyzz1tbK"
      }
    }
  ]
}